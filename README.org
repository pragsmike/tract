#+TITLE: Tract

* Tract: Extract content

  Text, image, video link, and other links.

  Focus on Substack to begin with.

  Input: Author, date range
  Output: For each post, a directory
          * body text
          * for each image
            * image, caption
          * for each link
            * href
            * anchor text


* Install chrome and chrome driver

  #+begin_src bash
  sudo apt update && sudo apt upgrade -y
  #+end_src

  #+begin_src bash
  sudo apt install wget curl gnupg -y
  sudo apt install libgbm1 libu2f-udev libwayland-server0
  #+end_src

  #+begin_src bash
  wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
  #+end_src

  #+begin_src bash
  sudo apt install ./google-chrome-stable_current_amd64.deb
  #+end_src

  #+begin_src bash
    google-chrome --version
    google-chrome --no-sandbox
    google-chrome --headless --disable-gpu --screenshot https://www.chromestatus.com/
  #+end_src

** chrome driver
  Crucial Prerequisite: WebDriver
  Etaoin doesn't control the browser directly. It speaks the "WebDriver" protocol to a small executable provided by the browser vendor. You need to install this driver.
  Recommendation: Use chromedriver, as it's the most common.

*** How to Install:
  * Find your Chrome browser version (Go to chrome://settings/help).
  * Download the matching chromedriver version from the Chrome for Testing
    availability dashboard.
  * Unzip the downloaded file.
  * Place the chromedriver executable (or chromedriver.exe on Windows) in a
    directory that is on your system's PATH. (On macOS/Linux, /usr/local/bin is
    a common choice).

* Libraries

** Etaoin
      WebDriver protocol implementation:
     * Chrome/Firefox automation with human-like interaction timings
     * Screenshot-based debugging for anti-bot challenges

** Abrade for JavaScript rendering when required
      HtmlUnit-based scraper for JS-heavy Substack pages:
     * Full browser emulation with CSS/JS execution
     * Headless mode support for server environments

** Hato/clj-http for HTTP with rotating proxies

** Throttler + Safely for adaptive rate control

** Reaver for HTML parsing
    * JSoup wrapper optimized for data extraction from HTML:
    * CSS selector syntax with extract-from macro
    * Converts HTML tables to Clojure vectors automatically
    * Handles malformed HTML common in older Substack themes

    Substack scraping example:
      #+begin_src clojure
      (extract-from (reaver/parse html-content)
        ".post" [:title :content :date]
        "h1.post-title" text
        "div.post-content" text
        "time.post-date" (attr :datetime))
      #+end_src

** Enlive for HTML parsing
      Templating-focused library with powerful selector engine:
      * select function uses CSS-like syntax for DOM traversal
      * Built-in text extraction and attribute parsing
      * Used in production by large-scale scrapers handling 10k+ pages/day

* Snippets

  #+begin_src clojure
  (def driver (etaoin/start-chrome))
  (etaoin/go driver "https://substack.com")
  (let [posts (etaoin/query-all driver {:class "post"})]
    (map etaoin/get-text-el posts))
  #+end_src

* Substack-Specific Considerations
** Architecture Patterns
*** Incremental scraping:

  * Use Substackâ€™s /archive pagination with exponential backoff
  * Track Last-Modified headers to avoid redundant requests

*** Atom feed parsing:

    #+begin_src clojure
    (feed/parse-substack-feed "https://user.substack.com/feed")
    #+end_src

*** Image handling:
  * Lazy-load avoidance with window.scroll emulation
  * Referer header spoofing for hotlinked assets
