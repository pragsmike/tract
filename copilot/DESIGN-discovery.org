#+TITLE: Expanding the set of stored articles

Having now built a way to scrape certain web content, we turn now to ways to
manage and expand that content.

We have already implemented the mechanism for fetching a given set of
articles as HTML, and extracting Markdown and image content from them.
We'll call this the "scraping" part.

Here we describe a mechanism for expanding the set of articles that we
have downloaded and processed, by inspecting the existing set for references
to articles that we don't already have.  We then create a job listing those
URLs so that the existing scraping machinery will fetch and process them.
We'll call this the "discovery" part.

This new discovery mechanism will be a standalone tool, rather than
being a stage in the existing pipeline.
Its code will be largely separate from the existing scraping
code, though its namespace files will reside in the same codebase and git repo.
The two parts will operate on the same files in the existing "work" directory
structure.

The discovery may find URLs of articles that we have already downloaded and processed.
We will not download those again.  Ways to handle that are discussed below.

* Discovering URLs to Other Articles

  We will build a utility that will scan the article HTML documents already
  downloaded, and extract the URLs from them. We will derive a list of
  interesting URLs (defined below).

  Substack article content, or the surrounding matter, usually contains links, some of which are of interest.
  These URLs appear in the href attribute of A (anchor) tags, and less often in form and input elements.


** Links to Substack

    Sometimes an article's own content will contain links to other Substack articles.
    Also, an article's content will typically be presented along with a section of links
    to related articles by the same author, or articles by other Substack authors.
    All these are the ones we are immediately interested in listing.

** Links to external resources

    An article's own content may have links to external resources, not on Substack,
    such as wikipedia entries, youtube videos, or other web sites.
    These links may be references to source materials to support the arguments
    made in the article, relevant further reading, or that otherwise amplify the article's effect.

    We are interested in cataloging these, but we won't download them.

    A simple and effective solution would be to have the discovery tool produce
    a second database, with columns like source_article_key, external_url, link_text.

    For now it is acceptable to store these in a file.
    Later, we might use a lightweight SQL database such as SQLite.

** Ad links

    Ad links are to sponsored content.  Whenever we can recognize these, we will ignore them.

** Navigation, display, and subscription controls

    Anchor tags are also used for "Share", "Subscribe", and similar controls.
    Whenever we can recognize these, we will ignore them.

* Avoiding downloading articles more than once

  Ideally, we want to avoid downloading articles more than once.

  However, we realize that we might inadvertantly do so, for instance, if the
  same article is reachable by a different URL, or for some other reason we
  don't recognize it as being one that we already have, before downloading it.

  In any case, we especially want to avoid processing the article more than
  once, in case it does get downloaded again.

  One way to handle this is to keep a database of URLs that have already been
  downloaded. This database could be built by scanning
     * metadata of the articles we have stored as markdown
     * files in work/fetch/done to catch any unprocessed URLs that have been fetched
     * files in work/fetch/pending directories to catch URLs scheduled to be fetched

  We could also scan the HTML files that are pending parsing, but that adds
  complication to handle an edge case that won't happen often and at worst would
  cause an infrequent duplicate fetch.

  The discovery mechanism would then not submit for fetch
  any URL that appears in that "already-fetched" database.

  For now it is acceptable to store these in a file.
  Later, we might use a lightweight SQL database such as SQLite.

* Databases

  The discovery tool produces the databases below.
  Initially, these will be files, as we describe here.
  A later step will use SQLite instead.

   * Known URLs Database (known-urls.txt): A simple text file containing one
     canonical URL per line. This is easy to read, write, and search.
   * External Links Database (external-links.csv): A CSV (Comma-Separated
     Values) file with three columns: source_article_key,link_text,external_url.
     This is a standard, portable format.

* Expected execution sequence

   The scraping pipeline runs with a job supplied by the user.
   This collects an initial set of articles and other files under the work directory.

   After the scraping pipeline has finished, the discovery tool runs. It does the following:
     * creates or updates the database of already-fetched URLs
     * creates or updates the database of referenced URLs
     * creates the fetch job for the next run of the scraping pipeline

   The user runs the scraping pipeline, which processes the new fetch job.

   In case the user starts the discovery tool while the scraping pipeline is active,
   it shouldn't cause any trouble.  Discovery will take note of any URLs that
   are in process, and won't include them in its fetch job.
   The worst outcome is that the scraping pipeline will not have fetched
   one or more of the HTML documents, so whatever URLs they may have referenced
   won't be included in the fetch job.  The next run of the discovery tool
   after the scraping pipeline has finished, will include them.

* Assumptions

   The discovery tool will not start the scraping pipeline, and viceversa.
   They are started manually by the user.

   The discovery tool will usually not run while the scraping pipeline is
   active, but it won't damage anything if it does. It might miss URLs
   referenced by articles not yet fetched, though, and in rare cases
   might cause a duplicate fetch, so it should print a warning
   if it notices that the scraping pipeline is running.

* Plan

  1) Scan HTML to produce two databases: referenced article URLs,
     and referenced external URLs, excluding those for ads, controls, etc.
     Initially, these databases will simply be stored in files.
     A later step will keep them in SQLite.

  2) Scan the files under the work directory to derive the list
     of already-downloaded URLs, and place them in a database.
     Again, this will initially be a file, but later will be SQLite.

  3) Generate a fetch job that has URLs that are of referenced articles,
     but which have not been downloaded already.
     This will be a YAML file placed in work/fetch/pending, with the key
     urls and a list of the URLs.
     The next time the pipeline runs, it will process that job.
