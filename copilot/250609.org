I'd like to structure the final workflow as a sequence of steps,
where the steps communicate via files in directories. Each step would have a
pair of input directories: pending and done. For example, the first step would
read a job file in its pending directory to find out what URLs are to be
fetched, fetch them, and put the HTML in the next step's pending directory where
the next step will find it, and then move the job file into its "done"
directory. This way, the existing code that reads from a file can still do that.
This makes it easy to test. In the future, we'll add the step that produces that
file by fetching from a URL, independently of the existing code. We'll work out
the details of the file-coordinated workflow later, including error handling.
For now, I just want to make sure our refactoring we're about to do remains
based on reading from files.

* Workflow coordinated by placing files in directories

Recall that the workflow coordination is done by files placed in directories.
Each stage has a pair of input directories, pending and done.
When the stage is activated, it looks for files in the pending directory and
acts on them.  It will write its output into a file in the pending directory of the next stage,
and only then will it move its input file from its own pending directory to its done directory.

If an error occurs when processing a file, that file will be moved to the stage's error directory,
along with a file that describes the error and how to correct it.

The intent is that the operation could be retried by moving the file from the error directory
back to the pending directory and activating the stage again.

** Job stage
  The first stage of the workflow is the job stage. It translates a jobspec
  file into a list of URLs in a file, one URL per line.
  The name of the jobspec file doesn't matter.

  The jobspec file can be one of
    * a literal list of URLs
      * The list is copied into the URL-per-line format in the fetch stage's pending directory.
    * the name of a Substack author and an optional date range
      * The URL of the author's Atom feed is derived from these
      * if substack supports the date range as URL parameters, the date range is encoded there
      * That Atom URL is placed in the job stage's own pending directory (same directory author/date file was in)
      * then the original author/date file is placed in the job stage's done directory
      * the job stage then activates itself so it will process the Atom URL file.
    * the URL of an Atom feed
      * The Atom document is fetched, URLs extracted into one-per-line format,
        and copied into the fetch stage's pending directory.
    * the URL of an RSS feed
      * The RSS document is fetched, URLs extracted into one-per-line format,
        and copied into the fetch stage's pending directory.

  The job stage reads a jobspec file from its pending directory, and writes a list of URLs to the
  pending directory of the fetch stage. Then it moves the jobspec file to its done directory.

** Fetch stage

  The fetch stage reads the URLs and writes HTML into the pending directory of the parser stage.
  Most of this stage is already implemented, though it needs to be modified to read
  the list of URLs from files in the fetch stage's pending directory and move them to the done directory
  after fetching them.

** Parser stage

  The parser stage reads HTML and writes markdown and images into the output directory.
  This stage is already implemented.
